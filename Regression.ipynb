{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hgc2eD54CPM"
      },
      "source": [
        "<h1 dir=\"rtl\" align=center>\n",
        "تخمین قیمت گوشی های تلفن همراه براساس آگهی دیوار\n",
        "</h1>\n",
        "<h2 dir=\"rtl\" align=center>\n",
        "محمدرضا عظیمی\n",
        "</h2>\n",
        "<h3 dir=\"rtl\" align=center>\n",
        "۸۱۰۱۹۶۵۱۰\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kKcR4zf4wNY"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h2>\n",
        "کلاس Regressor\n",
        "</h2>\n",
        "<p>\n",
        "میخواهیم با استفاده روش های هوش مصنوعی، تخمین قیمت تلفن های همراه را انجام دهیم. برای این کار کلاس Regressor\n",
        "را طراحی میکنیم.\n",
        "\n",
        "این کلاس داده های مورد نظر را از دیتا ست خوانده و با پیش پردازش و تمیز کردن آن ها و سپس استخراج ویژگی های مختلف، به یادگیری از روی داده ها می پردازد.\n",
        "</p>\n",
        "\n",
        "<h2>\n",
        "کتابخانه scikit learn\n",
        "</h2>\n",
        "<p>\n",
        "در این پروژه از کتابخانه scikit learn\n",
        "برای پردازش داده ها و یادگیری استفاده می کنیم.\n",
        "\n",
        "این کتابخانه توابع مناسبی جهت جدا کردن، پیش پردازش و یادگیری رگرسیون دارد که در ادامه از آن ها استفاده می کنیم.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ17copA5SjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "32e2b3fd-62da-4f9f-c7a3-86738cdc03af"
      },
      "source": [
        "!pip install hazm\n",
        "!pip install unidecode\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "import io\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy\n",
        "import operator\n",
        "import random\n",
        "from hazm import *\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "\n",
        "class Regressor:\n",
        "\tdef __init__(self):\n",
        "\t\tself.train = list()\n",
        "\t\tself.evaluation = list()\n",
        "\t\tself.X_train = list()\n",
        "\t\tself.X_test = list()\n",
        "\t\tself.y_train = list()\n",
        "\t\tself.y_test = list()\n",
        "\t\tself.reg = LinearRegression()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16RhDfP-5YYM"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h2>\n",
        "پیش پردازش\n",
        "</h2>\n",
        "\n",
        "<p>\n",
        "تابع clean\n",
        "که یک تابع static\n",
        "می باشد، یک جمله را بعنوان ورودی گرفته و عملیات تمیز کردن داده را روی آن انجام می دهد.\n",
        "\n",
        "در این تابع که از بسته ی فارسی هضم استفاده می کند، ابتدا جملات را نرمال سازی کرده و حروف بی ربط و اضافی را از آن پاک می کنیم.\n",
        "\n",
        "سپس کلمات هر جمله را جدا کرده و با استفاده از تابع lemmatization\n",
        "کلمه مورد نظر را به ریشه ی اصلی اش بازمی گردانیم.\n",
        "\n",
        "این کار باعث می شود تا از شلوغی داده ها کاسته شده و راحت تر بتوانیم داده ها را پردازش کنیم.\n",
        "</p>\n",
        "<h3>\n",
        "پیدا کردن ریشه ی کلمات\n",
        "</h3>\n",
        "\n",
        "<p>\n",
        "\n",
        "برای این کار می توانیم از دو روش stemming\n",
        "و یا lemmatization\n",
        "استفاده کنیم.\n",
        "<br>\n",
        "در روش stemming،\n",
        "به معنی کلمات توجه نمیشود و فقط بخش پایانی کلمات مانند 'ها' در 'آن ها' و پسوند های کلمات، صفات و افعال حذف میشوند.\n",
        "همچنین در این روش به اشکال مختلف افعال و زمان آن ها، توجه نمیشود. بنابراین این روش به خوبی ریشه ی کلمات را به دست نمیدهد.\n",
        "\n",
        "اما این روش سریع است\n",
        "\n",
        "اما در روش lemmatization به معنی کلمات توجه میشود و تنها درصورتی پسوند کلمات حذف میشوند که کلمه ی جدید با معنی باشد. در نتیجه کلماتی که از این روش بدست می آیند دارای معنی بوده و ریشه ی واقعی کلمه تولید میشود.\n",
        "البته ممکن است ریشه ی بعضی از کلمات را نتواند بدست بیاورد و خود کلمه رو برگرداند.\n",
        "\n",
        "از آنجاییکه متن شامل غلط های املایی بسیاری است، استفاده از lemmatization\n",
        "کمک زیادی به ما نمی کند و نمی تواند ریشه ی کلمات را بدست دهد.\n",
        "\n",
        "از این رو از stemmer استفاده میکنیم.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mn6sB1m65Ik"
      },
      "source": [
        "\t@staticmethod\n",
        "\tdef clean(sentence):\n",
        "\t\tnormalizer = Normalizer()\n",
        "\t\tsentence = normalizer.normalize(sentence)\n",
        "\t\twords = word_tokenize(sentence)\n",
        "\t\tnew_words = []\n",
        "\n",
        "\t\tfor word in words:\n",
        "\t\t\tnew_words.append(Stemmer().stem(word))\n",
        "\n",
        "\t\treturn new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Y7cs7_6_4d"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h3>\n",
        "ویژگی ها در مجموعه دادگان\n",
        "</h3>\n",
        "<p>\n",
        "در دیتاست موجود، ۷ ویژگی وجود دارد که ویژگی هشتم قیمت بوده و ما باید آن را تخمین بزنیم.\n",
        "\n",
        "۱. برند\n",
        "\n",
        "این ویژگی تاثیر مهمی در قیمت موبایل دارد و مقادیر این ستون محدود می باشند.\n",
        "\n",
        "بنابراین از روش label encoding\n",
        "استفاده کرده و به هر برند یک عدد اختصاص می دهیم.\n",
        "\n",
        "این روش در این پروژه از روش one hot encoding\n",
        "بهتر است زیرا برند ها باهم قابل مقایسه بوده و برخی از دیگری گران ترند.\n",
        "\n",
        "برای مثال برند اپل معمولا قیمت های بالایی دارد.\n",
        "\n",
        "۲. شهر\n",
        "\n",
        "شهر محل فروش تاثیر بسزایی در قیمت یک موبایل ندارد، اما می توان گفت که در شهرهای کم جمعیت ممکن است موبایل با قیمت متفاوتی نسبت به شهرهای پرجمعیت به فروش برسد.\n",
        "\n",
        "بنابراین برای ویژگی شهر نیز دقیقا مانند برند عمل کرده و به هر شهر عددی اختصاص می دهیم.\n",
        "\n",
        "۳. عنوان\n",
        "\n",
        "عنوان یک آگهی اطلاعات مفیدی را به ما می دهید.\n",
        "\n",
        "از جمله مهم ترین اطلاعاتی که می شود از عنوان برداشت کرد، مدل گوشی است.\n",
        "\n",
        "معمولا مدل موبایل در عنوان آگهی ذکر می شود که تاثیر بسزایی در قیمت آن موبایل دارد.\n",
        "\n",
        "برای مثال بین مدل های مختلف برند اپل تفاوت قیمت وجود دارد.\n",
        "\n",
        "همچنین در عنوان آگهی می توان اطلاعاتی درمورد ویژگی های سخت افزاری از جمله رم موبایل تیز بدست آورد.\n",
        "\n",
        "۴. توضیحات\n",
        "\n",
        "توضیحات نیز می تواند اطلاعاتی درمورد ویژگی های جزئی تر موبایل به ما بدهد.\n",
        "\n",
        "در این ستون علاوه بر اطلاعات موجود در ستون عنوان، اطلاعاتی مانند نو یا کهنه بودن، شکستگی، سلامت، گرانتی، اقلام همراه، جعبه، خط و خش و سایر ایرادها نیز بدست آورد.\n",
        "\n",
        "با پردازش این متن توضیحات میتوان اطلاعات مفیدی را بدست آورد.\n",
        "\n",
        "۵. تعداد تصاویر\n",
        "\n",
        "تعداد تصاویری که از یک کالا موجود است می تواند بر سالم و تمیز بودن کالا دلالت داشته باشد و در نتیجه در قیمت موثر باشد.\n",
        "\n",
        "۶. زمان ایجاد آگهی\n",
        "\n",
        "از آنجایی که زمان آگهی فقط روز هفته و ساعت را اعلام کرده، نمیتواند تاثیر زیادی بر قیمت موبایل داشته باشد.\n",
        "\n",
        "این ویژگی واریانس پایینی دارد و از آن صرف نظر می کنیم.\n",
        "\n",
        "۷. قیمت\n",
        "\n",
        "این ویژگی عددی غیر اعشاری است که ما باید آن را تخمین بزنیم.\n",
        "\n",
        "در برخی از سطرها قیمت موجود نیست و این سطرها باید از داده ها حذف شوند.\n",
        "\n",
        "همچنین در برخی از سطرها نیز چند صفر از جلوی قیمت برداشته شده است.\n",
        "\n",
        "این موارد نقص های دیتاست بوده و می بایست آن ها را برطرف و یا از مجموعه دادگان حذف کرد.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl15xzgRBJu-"
      },
      "source": [
        "\t@staticmethod\n",
        "\tdef extract_ram_model(sentence, brands):\n",
        "\t\tbrands = [b.lower() for b in brands]\n",
        "\t\tbrands.extend(['iphone', 'asus', 'galaxy', 'lumia', 'honor'])\n",
        "\t\tenglish = [ch for ch in sentence if (ord(ch) <= ord('9') and ord(ch) >= ord('0')) or ch is ' ' \\\n",
        "            or (ord(ch) <= ord('z') and ord(ch) >= ord('a')) or (ord(ch) <= ord('Z') and ord(ch) >= ord('A'))]\n",
        "\t\tenglish = ''.join(english).lower().split()\n",
        "\t\tenglish = [w for w in english if w not in brands]\n",
        "\t\t# print(english)\n",
        "\n",
        "\t\tram = 0\n",
        "\t\tflag = False\n",
        "\t\thas_ram = False\n",
        "\t\tfor word in english:\n",
        "\n",
        "\t\t\tfor brand in brands:\n",
        "\t\t\t\tif brand in word:\n",
        "\t\t\t\t\tflag = True\n",
        "\n",
        "\t\t\tif '16' in word:\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = 16\n",
        "\t\t\telif '32' in word:\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = 32\n",
        "\t\t\telif '64' in word:\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = 64\n",
        "\t\t\telif '128' in word:\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = 128\n",
        "\t\t\telif '256' in word:\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = 256\n",
        "\n",
        "\t\t\telif 'g' in word.lower():\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tif '4' in word:\n",
        "\t\t\t\t\tram = 4\n",
        "\t\t\t\telif '8' in word:\n",
        "\t\t\t\t\tram = 8\n",
        "\n",
        "\t\t\telif word.lower() == 'gig' or word.lower() == 'gb':\n",
        "\t\t\t\thas_ram = True\n",
        "\t\t\t\tram = max([int(num) for num in english if num.isdigit()])\n",
        "\n",
        "\t\t\tif has_ram:\n",
        "\t\t\t\tenglish.remove(word)\n",
        "\n",
        "\t\tif not has_ram:\n",
        "\t\t\tram = 0\n",
        "\t\t\tflag = False\n",
        "\t\t\thas_ram = False\n",
        "\t\t\tfor word in unidecode(''.join(english)):\n",
        "\n",
        "\t\t\t\tfor brand in brands:\n",
        "\t\t\t\t\tif brand in word:\n",
        "\t\t\t\t\t\tflag = True\n",
        "\n",
        "\t\t\t\tif '16' in word:\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = 16\n",
        "\t\t\t\telif '32' in word:\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = 32\n",
        "\t\t\t\telif '64' in word:\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = 64\n",
        "\t\t\t\telif '128' in word:\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = 128\n",
        "\t\t\t\telif '256' in word:\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = 256\n",
        "\n",
        "\t\t\t\telif 'گیگ' in word.lower():\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tif '4' in word:\n",
        "\t\t\t\t\t\tram = 4\n",
        "\t\t\t\t\telif '8' in word:\n",
        "\t\t\t\t\t\tram = 8\n",
        "\n",
        "\t\t\t\telif word.lower() == 'گیگ' or word.lower() == 'گیگابایت':\n",
        "\t\t\t\t\thas_ram = True\n",
        "\t\t\t\t\tram = max([int(num) for num in english if num.isdigit()])\n",
        "\n",
        "\t\t\t\tif has_ram:\n",
        "\t\t\t\t\tenglish.remove(word)\n",
        "\n",
        "\t\tmodel = 'unknown'\n",
        "\t\tif (flag and len(english) > 0):\n",
        "\t\t\tok = False\n",
        "\t\t\tfor br in brands:\n",
        "\t\t\t\tif br in english[0]:\n",
        "\t\t\t\t\tok = True\n",
        "\n",
        "\t\t\tif ok:\n",
        "\t\t\t\tmodel = english[0]\n",
        "\t\t\t\t# print(model)\n",
        "\n",
        "\t\treturn [ram, model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJThJQVBN0g"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h3>\n",
        "استخراج ویژگی ها\n",
        "</h3>\n",
        "<p>\n",
        "درمورد ویژگی هایی مانند برند و شهر پیش تر صحبت شد و استخراج آن ها بسادگی و با روش label encoding قابل انجام بود.\n",
        "\n",
        "اما برخی از ویژگی ها درون متن ها نهفته اند و باید آن ها را به دقت استخراج کنیم:\n",
        "\n",
        "۱. رم\n",
        "\n",
        "حجم رم یک موبایل تاثیر بسزایی در قیمت آن دارد.\n",
        "\n",
        "این اطلاعات در عنوان و یا توضیحات ذکر می شوند و می توان آن ها را استخراج کرد.\n",
        "\n",
        "برای این کار دنبال کلمات کلیدی مانند گیگ، gb، gig، ‌‌g و غیره \n",
        "می گردیم.\n",
        "\n",
        "درصورت یافتن این کلمات در متن، می توانیم حجم رم را در اطراف این کلمات پیدا کنیم.\n",
        "\n",
        "همچنین برخی از اعداد بصورت فارسی نوشته شده اند که با استفاده از تابع unidecode\n",
        "آن ها را به انگلیسی تبدیل می کنیم.\n",
        "\n",
        "۲. امتیاز\n",
        "\n",
        "یک ویژگی جدید بنام امتیاز تعریف می کنیم که با بر اساس تعداد کلمات مثبت و منفی موجودد در توضیحات آگهی تعیین می شود.\n",
        "\n",
        "برای مثال آگهی ای که دارای کلمات سالم بدون خط و .. است امتیاز مثبت و آگهی ای که شامل کلماتی از قبیل شکسته، ترک و...\n",
        "است امتیاز منفی می گیرد.\n",
        "\n",
        "۳. مدل گوشی\n",
        "\n",
        "مورد دیگری که در توضیحات پنهان است مدل گوشی می باشد.\n",
        "\n",
        "سعی کردیم تا مدل های مختلفی را بر اساس برند موبایل پیدا کرده و به هر کدام برچسبی بدهیم و آن را به یک ویژگی تبدیل کنیم.\n",
        "\n",
        "در ادامه تابع پیش پردازش را مشاهده می کنیم.\n",
        "</p>\n",
        "<h3>\n",
        "scale کردن ویژگی ها\n",
        "</h3>\n",
        "<p>\n",
        "برای بهتر شدن دقت و استاندارد شدن داده ها، آن ها را بر اساس میانگین و واریانس نرمال کرده و سپس از آن ها استفاده می کنیم.\n",
        "\n",
        "این کار را با استفاده از تابع StandardScaler\n",
        "موجود در بسته ی scikit learn\n",
        "انجام می دهیم.\n",
        "</p>\n",
        "\n",
        "<h3>\n",
        "جدا کردن داده های تمرین و تست\n",
        "</h3>\n",
        "<p>\n",
        "با استفاده از تابع train_test_split، ۸۰ درصد\n",
        "از داده ها را برای یادگیری و ۲۰ درصد را برای تست جدا می کنیم.\n",
        "\n",
        "این کار را بصورت تصادفی و توسط همین تابع انجام می دهیم.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5sg4VqbF5cN"
      },
      "source": [
        "\tdef pre_process(self, data_file):\n",
        "\t\tdf = pd.read_csv(data_file)\n",
        "\t\tdata = df.loc[(df['price'] > 1000)]\n",
        "\t\tcities = set(data['city'])\n",
        "\t\tbrands = set(data['brand'])\n",
        "\t\tbrands = set([b.split(':')[0] for b in brands])\n",
        "\t\tcity_dict = {city : 0 for city in cities}\n",
        "\t\tbrand_dict = {brand : 0 for brand in brands}\n",
        "\n",
        "\t\ti = 0\n",
        "\t\tfor k, _ in city_dict.items():\n",
        "\t\t\tcity_dict[k] = i\n",
        "\t\t\ti += 1\n",
        "\n",
        "\t\ti = 0\n",
        "\t\tfor k, _ in brand_dict.items():\n",
        "\t\t\tbrand_dict[k] = i\n",
        "\t\t\ti += 1\n",
        "\n",
        "\t\tdata.insert(2, 'ram', np.zeros(len(data)), True)\n",
        "\t\tdata.insert(2, 'rate', np.zeros(len(data)), True)\n",
        "\t\tdata.insert(2, 'model', ['' for i in range(len(data))], True)\n",
        "\t\tfor i, _ in data.iterrows():\n",
        "\t\t\tdata.at[i, 'city'] = city_dict[data['city'][i]]\n",
        "\t\t\tdata.at[i, 'brand'] = brand_dict[data['brand'][i].split(':')[0]]\n",
        "\t\t\ttitle = data.at[i, 'title']\n",
        "\t\t\tdesc = data.at[i, 'desc']\n",
        "\t\t\tt_d = title + desc\n",
        "\t\t\tram_model = self.extract_ram_model(t_d, brands)\n",
        "\t\t\tdata.at[i, 'ram'] = ram_model[0]\n",
        "\t\t\tdata.at[i, 'model'] = ram_model[1]\n",
        "\t\t\ttitle = self.clean(title)\n",
        "\t\t\tinfo = title.copy()\n",
        "\t\t\tinfo.extend(self.clean(desc))\n",
        "\t\t\tgoods = ['سالم', 'نو', 'اک', 'آک', 'درحد', 'تمیز', 'بسیار', 'کیف', 'هنذفری', 'پلمپ', 'بدون', 'سالمه', 'فابریک', 'شارژر',\\\n",
        "                    'تعمیرنرفته', 'کاملا', 'چرم', 'درحداک', 'کارتون', 'کارتن', 'تست', 'گارانتی', 'اورجینال', 'خط']\n",
        "\t\t\tbads = ['شکسته', 'ایراد', 'ایرادش', 'شکست', 'تاچ', 'افتاده', 'ترک', 'خوردگ', 'خوردگی', 'افتاد']\n",
        "\n",
        "\t\t\trate = 0.0\n",
        "\t\t\tfor word in info:\n",
        "\t\t\t\tif word in goods:\n",
        "\t\t\t\t\trate += 1\n",
        "\t\t\t\telif word in bads:\n",
        "\t\t\t\t\trate -= 1\n",
        "\n",
        "\t\t\tdata.at[i, 'rate'] = rate\n",
        "\n",
        "\t\tmodels = set(data['model'])\n",
        "\t\tmodel_dict = {model : 0 for model in models}\n",
        "\t\ti = 0\n",
        "\t\tfor k, _ in model_dict.items():\n",
        "\t\t\tmodel_dict[k] = i\n",
        "\t\t\ti += 1\n",
        "\n",
        "\t\tfor i, _ in data.iterrows():\n",
        "\t\t\tdata.at[i, 'model'] = model_dict[data['model'][i]]\n",
        "\n",
        "\t\tscaler = StandardScaler()\n",
        "\t\tscaler.fit(data[['brand', 'city', 'image_count', 'ram', 'rate', 'model']])\n",
        "\t\tscaled_data = scaler.transform(data[['brand', 'city', 'image_count', 'ram', 'rate', 'model']])\n",
        "\n",
        "\t\tself.X_train, self.X_test, self.y_train, self.y_test = train_test_split(scaled_data, data['price'], test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_s1AicUF_Kw"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h2>\n",
        "یادگیری و تست\n",
        "</h2>\n",
        "<p>\n",
        "برای یادگیری از LinearRegression\n",
        "استفاده می کنیم.\n",
        "\n",
        "تابع fit این کلاس ویژگی\n",
        "های ما را گرفته و مدلی آموزش دیده شده را تولید می کند.\n",
        "\n",
        "همچنین با استفاده از تابع predict\n",
        "قیمت را برای داده های تست خود نیز محاسبه کرده و در نهایت خطا را بصورت مجموع قدر مطلق خطاها بدست آورده و نتیجه کار خود را مشاهده میکنیم:\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCSFmZtIFtj"
      },
      "source": [
        "\tdef learn(self):\n",
        "\t\tself.reg.fit(self.X_train, self.y_train)\n",
        "\t\ty_pred = self.reg.predict(self.X_test)\n",
        "\n",
        "\t\tloss = 0\n",
        "\t\tfor x, y in zip(y_pred, self.y_test):\n",
        "\t\t\tloss += abs(x - y)\n",
        "\n",
        "\t\tprint('Loss:', loss / len(self.y_test))\n",
        "\n",
        "\tX_train : list\n",
        "\tX_test : list\n",
        "\ty_train : list\n",
        "\ty_test : list\n",
        "\treg : LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FVbfi1dIHk9"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h2>\n",
        "مشاهده نتیجه\n",
        "</h2>\n",
        "<p>\n",
        "اکنون شی ای از کلاس Regressor\n",
        "ساخته و توابع را فراخوانی میکنیم تا مقدار خطا را مشاهده کنیم.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgKoMghlIRqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a660cc80-a7ac-4d9e-c517-c013d4649e84"
      },
      "source": [
        "def main():\n",
        "\tregressor = Regressor()\n",
        "\tregressor.pre_process(open(\"mobile_phone_dataset.csv\", newline=''))\n",
        "\tregressor.learn()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 354344.894938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "672ROgweIYC_"
      },
      "source": [
        "<div lang=\"fa\" dir=\"rtl\">\n",
        "<h2>\n",
        "تحلیل و بررسی\n",
        "</h2>\n",
        "<p>\n",
        "همانطور که مشاهده می کنیم، تخمینگر ما حدودا ۳۵۰ هزار تومان خطا داشته که خیلی دقیق نیست اما تقریب خوبی از قیمت به ما می دهد.\n",
        "\n",
        "از دلایل عدم موفقیت کامل می توان به موارد زیر اشاره کرد:\n",
        "\n",
        "۱. دیتاست نامرتب\n",
        "\n",
        "۲. ترکیب کلمات و حروف فارسی و انگلیسی\n",
        "\n",
        "۳. عدم بیان ویژگی های مشخص در برخی آگهی ها\n",
        "\n",
        "۴. نقص های داده ای مانند قیمت های پرت\n",
        "</p>\n",
        "\n",
        "<h2>\n",
        "کارهای قابل انجام\n",
        "</h2>\n",
        "<p>\n",
        "زمان در این پروژه محدود بود اما در آینده می توان برخی اعمال را جهت بهتر کردن دقت انجام داد.\n",
        "\n",
        "این اعمال عبارتند از:\n",
        "\n",
        "۱. حذف یا اصلاح برخی قیمت های پرت\n",
        "\n",
        "برای مثال قیمت های خیلی پایین را حذف کرده و یا در ۱۰۰۰ ضرب کنیم.\n",
        "\n",
        "۲. بررسی سایر مشخصات سخت افزاری\n",
        "\n",
        "در اینجا ما فقط رم را بررسی کردیم درحالیکه میتوان سایر موارد مانند حافظه جانبی، دوربین و .. را نیز از توضیحات بیرون کشید و آن ها را بررسی کرد.\n",
        "\n",
        "۳. استخراج دقیقتر مدل گوشی\n",
        "\n",
        "مدل یک گوشی تعیین کننده ترین معیار برای قیمت آن پس از برند است.\n",
        "\n",
        "از آنجاییکه این مورد به راحتی از دادگان قابل استخراج نیست، می توان با دقت بیشتری و یا جمع آوری تمامی مدل های ممکن از گوشی، مدل را نیز بصورت دقیق تر استخراج کرد.\n",
        "\n",
        "۴. استفاده از روش های پردازش متن\n",
        "\n",
        "برخی روش ها مانند tf-idf\n",
        "می توانند ما را در تشخیص کلمات مهم و تاثیر گذار در قبمت یاری کرده و دقت مدل را بهبود بخشند.\n",
        "</p>\n",
        "</div>"
      ]
    }
  ]
}